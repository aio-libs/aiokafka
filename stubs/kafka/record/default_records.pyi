from kafka.codec import gzip_decode as gzip_decode, gzip_encode as gzip_encode, lz4_decode as lz4_decode, lz4_encode as lz4_encode, snappy_decode as snappy_decode, snappy_encode as snappy_encode
from kafka.errors import CorruptRecordException as CorruptRecordException, UnsupportedCodecError as UnsupportedCodecError
from kafka.record.abc import ABCRecord as ABCRecord, ABCRecordBatch as ABCRecordBatch, ABCRecordBatchBuilder as ABCRecordBatchBuilder
from kafka.record.util import calc_crc32c as calc_crc32c, decode_varint as decode_varint, encode_varint as encode_varint, size_of_varint as size_of_varint
from typing import Any, Optional

class DefaultRecordBase:
    HEADER_STRUCT: Any = ...
    ATTRIBUTES_OFFSET: Any = ...
    CRC_OFFSET: Any = ...
    AFTER_LEN_OFFSET: Any = ...
    CODEC_MASK: int = ...
    CODEC_NONE: int = ...
    CODEC_GZIP: int = ...
    CODEC_SNAPPY: int = ...
    CODEC_LZ4: int = ...
    TIMESTAMP_TYPE_MASK: int = ...
    TRANSACTIONAL_MASK: int = ...
    CONTROL_MASK: int = ...
    LOG_APPEND_TIME: int = ...
    CREATE_TIME: int = ...

class DefaultRecordBatch(DefaultRecordBase, ABCRecordBatch):
    def __init__(self, buffer: Any) -> None: ...
    @property
    def base_offset(self): ...
    @property
    def magic(self): ...
    @property
    def crc(self): ...
    @property
    def attributes(self): ...
    @property
    def last_offset_delta(self): ...
    @property
    def compression_type(self): ...
    @property
    def timestamp_type(self): ...
    @property
    def is_transactional(self): ...
    @property
    def is_control_batch(self): ...
    @property
    def first_timestamp(self): ...
    @property
    def max_timestamp(self): ...
    def __iter__(self) -> Any: ...
    def __next__(self): ...
    next: Any = ...
    def validate_crc(self): ...

class DefaultRecord(ABCRecord):
    def __init__(self, offset: Any, timestamp: Any, timestamp_type: Any, key: Any, value: Any, headers: Any) -> None: ...
    @property
    def offset(self): ...
    @property
    def timestamp(self): ...
    @property
    def timestamp_type(self): ...
    @property
    def key(self): ...
    @property
    def value(self): ...
    @property
    def headers(self): ...
    @property
    def checksum(self) -> None: ...

class DefaultRecordBatchBuilder(DefaultRecordBase, ABCRecordBatchBuilder):
    MAX_RECORD_OVERHEAD: int = ...
    def __init__(self, magic: Any, compression_type: Any, is_transactional: Any, producer_id: Any, producer_epoch: Any, base_sequence: Any, batch_size: Any) -> None: ...
    def append(self, offset: Any, timestamp: Any, key: Any, value: Any, headers: Optional[Any] = ...): ...
    def write_header(self, use_compression_type: bool = ...) -> None: ...
    def build(self): ...
    def size(self): ...
    def size_in_bytes(self, offset: Any, timestamp: Any, key: Any, value: Any, headers: Any): ...
    @classmethod
    def size_of(cls, key: Any, value: Any, headers: Any): ...
    @classmethod
    def estimate_size_in_bytes(cls, key: Any, value: Any, headers: Any): ...

class DefaultRecordMetadata:
    def __init__(self, offset: Any, size: Any, timestamp: Any) -> None: ...
    @property
    def offset(self): ...
    @property
    def crc(self) -> None: ...
    @property
    def size(self): ...
    @property
    def timestamp(self): ...
